{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trI2fh745oAW",
        "outputId": "54e1d8fb-a561-43dc-ccf6-c2ab3fc98d01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.12/dist-packages (0.11.9)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.26.7)\n",
            "Requirement already satisfied: ebooklib in /usr/local/lib/python3.12/dist-packages (0.20)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Requirement already satisfied: pdfminer.six==20251230 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (20251230)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (5.3.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251230->pdfplumber) (3.4.4)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251230->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from ebooklib) (6.0.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from ebooklib) (1.17.0)\n",
            "Requirement already satisfied: typing_extensions>=4.9.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (4.15.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20251230->pdfplumber) (3.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install pdfplumber pymupdf ebooklib python-docx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdfplumber\n",
        "import fitz  # PyMuPDF\n",
        "from ebooklib import epub\n",
        "from docx import Document\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "    return text\n",
        "\n",
        "\n",
        "def extract_text_from_pdf_pymupdf(file_path):\n",
        "    text = \"\"\n",
        "    doc = fitz.open(file_path)\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "\n",
        "def extract_text_from_epub(file_path):\n",
        "    book = epub.read_epub(file_path)\n",
        "    text = \"\"\n",
        "    for item in book.get_items():\n",
        "        if item.get_type() == epub.ITEM_DOCUMENT:\n",
        "            text += item.get_content().decode(\"utf-8\", errors=\"ignore\")\n",
        "    return text\n",
        "\n",
        "\n",
        "def extract_text_from_txt(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        return f.read()\n",
        "\n",
        "\n",
        "def extract_text_from_docx(file_path):\n",
        "    doc = Document(file_path)\n",
        "    return \"\\n\".join([para.text for para in doc.paragraphs])\n",
        "\n",
        "\n",
        "def load_book(file_path):\n",
        "    extension = os.path.splitext(file_path)[1].lower()\n",
        "\n",
        "    if extension == \".pdf\":\n",
        "        return extract_text_from_pdf(file_path)\n",
        "        # OR use PyMuPDF:\n",
        "        # return extract_text_from_pdf_pymupdf(file_path)\n",
        "\n",
        "    elif extension == \".epub\":\n",
        "        return extract_text_from_epub(file_path)\n",
        "\n",
        "    elif extension == \".txt\":\n",
        "        return extract_text_from_txt(file_path)\n",
        "\n",
        "    elif extension == \".docx\":\n",
        "        return extract_text_from_docx(file_path)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format\")\n"
      ],
      "metadata": {
        "id": "Ce7lP8mK53-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "\n",
        "file_path = \"/content/Introduction to Machine Learning with Python.pdf\"\n",
        "\n",
        "# Load and extract text\n",
        "raw_text = \"\"\n",
        "with pdfplumber.open(file_path) as pdf:\n",
        "    for page in pdf.pages:\n",
        "        raw_text += page.extract_text()\n",
        "\n",
        "print(raw_text[:2000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CPb7qqB6V48",
        "outputId": "ebaef0b8-7dea-4e86-ccac-e932f0af2df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Introduction to\n",
            "Machine\n",
            "Learning\n",
            "with Python\n",
            "A GUIDE FOR DATA SCIENTISTS\n",
            "Andreas C. Müller & Sarah GuidoIntroduction to Machine Learning\n",
            "with Python\n",
            "A Guide for Data Scientists\n",
            "Andreas C. Müller and Sarah Guido\n",
            "BBeeiijjiinngg BBoossttoonn FFaarrnnhhaamm SSeebbaassttooppooll TTookkyyooIntroduction to Machine Learning with Python\n",
            "by Andreas C. Müller and Sarah Guido\n",
            "Copyright © 2017 Sarah Guido, Andreas Müller. All rights reserved.\n",
            "Printed in the United States of America.\n",
            "Published by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\n",
            "O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\n",
            "also available for most titles (http://safaribooksonline.com). For more information, contact our corporate/\n",
            "institutional sales department: 800-998-9938 or corporate@oreilly.com.\n",
            "Editor: Dawn Schanafelt Indexer: Judy McConville\n",
            "Production Editor: Kristen Brown Interior Designer: David Futato\n",
            "Copyeditor: Rachel Head Cover Designer: Karen Montgomery\n",
            "Proofreader: Jasmine Kwityn Illustrator: Rebecca Demarest\n",
            "October 2016: First Edition\n",
            "Revision History for the First Edition\n",
            "2016-09-22: First Release\n",
            "See http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details.\n",
            "The O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Introduction to Machine Learning with\n",
            "Python, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\n",
            "While the publisher and the authors have used good faith efforts to ensure that the information and\n",
            "instructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\n",
            "for errors or omissions, including without limitation responsibility for damages resulting from the use of\n",
            "or reliance on this work. Use of the information and instructions contained in this work is at your own\n",
            "risk. If any code samples or other technology this work contains or describes is subject to open source\n",
            "licenses or the intellectual pr\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**text_preprocessor.py**"
      ],
      "metadata": {
        "id": "fBIF579K9FfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import List, Dict\n",
        "\n",
        "\n",
        "def normalize_whitespace(text: str) -> str:\n",
        "    \"\"\"Normalize extra spaces, tabs, and newlines.\"\"\"\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "\n",
        "def remove_page_numbers(text: str) -> str:\n",
        "    \"\"\"Remove standalone page numbers.\"\"\"\n",
        "    text = re.sub(r'\\n?\\s*\\d+\\s*\\n', '\\n', text)\n",
        "    return text\n",
        "\n",
        "\n",
        "def remove_headers_and_footers(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Removes common header/footer patterns:\n",
        "    - Repeated short lines\n",
        "    - Page indicators\n",
        "    \"\"\"\n",
        "    lines = text.splitlines()\n",
        "    cleaned_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        if len(line.strip()) < 5:\n",
        "            continue\n",
        "        if re.match(r'page\\s*\\d+', line.lower()):\n",
        "            continue\n",
        "        cleaned_lines.append(line)\n",
        "\n",
        "    return \"\\n\".join(cleaned_lines)\n",
        "\n",
        "\n",
        "def remove_references(text: str) -> str:\n",
        "    \"\"\"Remove references or bibliography sections.\"\"\"\n",
        "    patterns = [\n",
        "        r'\\nreferences\\b.*',\n",
        "        r'\\nbibliography\\b.*',\n",
        "        r'\\nworks cited\\b.*'\n",
        "    ]\n",
        "\n",
        "    for pattern in patterns:\n",
        "        text = re.sub(pattern, '', text, flags=re.IGNORECASE | re.DOTALL)\n",
        "\n",
        "    return text\n",
        "\n",
        "\n",
        "def clean_text(raw_text: str) -> str:\n",
        "    \"\"\"Full cleaning pipeline.\"\"\"\n",
        "    text = raw_text\n",
        "    text = remove_headers_and_footers(text)\n",
        "    text = remove_page_numbers(text)\n",
        "    text = remove_references(text)\n",
        "    text = normalize_whitespace(text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "Egtb7ly28SmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**chapter_detector.py**"
      ],
      "metadata": {
        "id": "3N0OwYZ99OSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import Dict\n",
        "\n",
        "\n",
        "CHAPTER_PATTERNS = [\n",
        "    r'chapter\\s+\\d+',\n",
        "    r'chapter\\s+[ivxlcdm]+',\n",
        "    r'\\b\\d+\\.\\s+[A-Z]',\n",
        "    r'\\b[A-Z][A-Z\\s]{5,}'\n",
        "]\n",
        "\n",
        "def split_by_chapter(text: str) -> Dict[str, str]:\n",
        "    \"\"\"\n",
        "    Splits text into chapters using regex patterns.\n",
        "    Returns a dict: {chapter_title: chapter_text}\n",
        "    \"\"\"\n",
        "    # Refined regex for Roman numerals to be more specific for chapters\n",
        "    roman_numeral_pattern = r'\\b(?:I|II|III|IV|V|VI|VII|VIII|IX|X|XI|XII|XIII|XIV|XV|XVI|XVII|XVIII|XIX|XX)\\b'\n",
        "\n",
        "    matches = list(re.finditer(\n",
        "        rf'(chapter\\s+\\d+|chapter\\s+{roman_numeral_pattern})',\n",
        "        text,\n",
        "        flags=re.IGNORECASE\n",
        "    ))\n",
        "\n",
        "    chapters = {}\n",
        "\n",
        "    if not matches:\n",
        "        chapters[\"Full Text\"] = text\n",
        "        return chapters\n",
        "\n",
        "    for i, match in enumerate(matches):\n",
        "        start = match.start()\n",
        "        end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n",
        "        chapter_title = match.group().title()\n",
        "        chapter_content = text[start:end].strip()\n",
        "        chapters[chapter_title] = chapter_content\n",
        "\n",
        "    return chapters"
      ],
      "metadata": {
        "id": "nq87JntD9LWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_text = load_book(file_path)\n",
        "cleaned_text = clean_text(raw_text)\n",
        "chapters = split_by_chapter(cleaned_text)\n",
        "\n",
        "print(f\"Detected chapters: {len(chapters)}\")\n",
        "for title, content in chapters.items():\n",
        "    print(title, \"->\", len(content), \"characters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox5d8jA19Tsl",
        "outputId": "ce0e9a37-ba76-4d28-8bb1-b0ac9762798c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected chapters: 8\n",
            "Chapter 1 -> 2265 characters\n",
            "Chapter 4 -> 1680 characters\n",
            "Chapter 5 -> 2407 characters\n",
            "Chapter 6 -> 689 characters\n",
            "Chapter 7 -> 1912 characters\n",
            "Chapter 8 -> 22723 characters\n",
            "Chapter 2 -> 867 characters\n",
            "Chapter 3 -> 2754 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Need to check with the Title Names, since we are only getting Chapter 1 That's it"
      ],
      "metadata": {
        "id": "gnAShpndBG3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain tiktoken langchain-text-splitters"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZxP_Rm-9XM-",
        "outputId": "3640a54a-72c9-416c-9bab-e40956668a17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: langchain-text-splitters in /usr/local/lib/python3.12/dist-packages (1.1.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.8 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.8)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.5)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (0.6.6)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (0.14.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (0.3.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.8->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.7->langchain) (1.12.2)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain) (0.25.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (4.12.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (0.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**chunker_langchain.py**"
      ],
      "metadata": {
        "id": "3DuKIz1iBf3p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "import tiktoken\n",
        "\n",
        "\n",
        "def chunk_text_langchain(\n",
        "    text: str,\n",
        "    chunk_size: int = 1000,\n",
        "    chunk_overlap: int = 200,\n",
        "    model_name: str = \"gpt-4\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Token-aware chunking using LangChain.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizer = tiktoken.encoding_for_model(model_name)\n",
        "\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        length_function=lambda text: len(tokenizer.encode(text)),\n",
        "        separators=[\"\\n\\n\", \"\\n\", \".\", \" \", \"\"]\n",
        "    )\n",
        "\n",
        "    chunks = splitter.split_text(text)\n",
        "    return chunks"
      ],
      "metadata": {
        "id": "ysFO_s1WBc7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fb508e51",
        "outputId": "a4645a16-5d2e-496a-d18e-2c561f637ee9"
      },
      "source": [
        "# Note: Triton installation can be complex and may require specific CUDA versions and environment setups.\n",
        "# The following command is a starting point, but further troubleshooting might be needed depending on your system.\n",
        "!pip install triton"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: triton in /usr/local/lib/python3.12/dist-packages (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "chunked_chapters = {}\n",
        "\n",
        "for chapter_title, chapter_text in chapters.items():\n",
        "    chunks = chunk_text_langchain(chapter_text)\n",
        "    chunked_chapters[chapter_title] = chunks\n",
        "\n",
        "print(len(chunked_chapters[\"Chapter 1\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyvgGEt9BjLY",
        "outputId": "dc67f154-1ed8-4ed1-b3fa-cc776b6c6d9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation:\n",
        "< 1000 tokens → ✅ 1 chunk (correct)\n",
        "> 1000 tokens → ❌ Should split → investigate"
      ],
      "metadata": {
        "id": "ZHOklznED4CA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "tokenizer = tiktoken.encoding_for_model(\"gpt-4\")\n",
        "token_count = len(tokenizer.encode(chapters[\"Chapter 1\"]))\n",
        "\n",
        "print(\"Token count:\", token_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPHoTcZPCmOp",
        "outputId": "1e1952c9-abba-4e83-85dd-f023a363c4fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token count: 590\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summarization Agent"
      ],
      "metadata": {
        "id": "l1ySx3wDimi5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# summarization_agent.py"
      ],
      "metadata": {
        "id": "oZQw2pawitvk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvmUnSBDi8tJ",
        "outputId": "e68e81ec-2fc0-4d2a-a453-177072179775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.16.0)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.8 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.2.8)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (0.6.6)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (9.1.2)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.8->langchain) (0.14.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (0.3.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (3.6.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.8->langchain) (3.0.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.7->langchain) (1.12.2)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.12/dist-packages (from langgraph-sdk<0.4.0,>=0.3.0->langgraph<1.1.0,>=1.0.7->langchain) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain) (0.25.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.8->langchain) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ],
      "metadata": {
        "id": "NExZG1E1jCOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-openai langchain-core openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUVCNuPbmPyx",
        "outputId": "606c343c-f292-4125-d370-fa987d1e606f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.12/dist-packages (1.1.7)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.8)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (2.16.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.12.3)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.6.6)\n",
            "Requirement already satisfied: packaging>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.14.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.12.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<5.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (4.0.0)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.7 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (1.0.7)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (0.3.3)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.7->langchain) (3.6.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.5)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<5.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.7->langchain) (1.12.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a18df75"
      },
      "source": [
        "# First, set your OpenRouter API key as an environment variable.\n",
        "# Replace 'sk-or-v1-YOUR_OPENROUTER_API_KEY_HERE' with your actual OpenRouter key.\n",
        "# You can store this in Colab secrets for better security.\n",
        "import os\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = \"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\"\n",
        "\n",
        "# Or if you want to use the API key directly in the code (not recommended for production):\n",
        "OPENROUTER_API_KEY = \"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\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "id": "90bec7f3",
        "outputId": "4e22d944-e309-4774-ca9c-1da5b95238f0"
      },
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "# Initialize the OpenAI client with OpenRouter's base URL and your API key\n",
        "client = OpenAI(\n",
        "  base_url=\"https://openrouter.ai/api/v1\",\n",
        "  api_key=os.environ.get(\"OPENROUTER_API_KEY\"), # or use OPENROUTER_API_KEY variable if set directly\n",
        ")\n",
        "\n",
        "# Make a chat completion request\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"tngtech/deepseek-r1t2-chimera:free\", # The model from your JavaScript example\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": \"What is the meaning of life?\",\n",
        "    },\n",
        "  ],\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message.content)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1770249600000'}, 'provider_name': None}}, 'user_id': 'user_38yXbtpb817LkWFrDul8PhcJH7u'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2598958685.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Make a chat completion request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m completion = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tngtech/deepseek-r1t2-chimera:free\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# The model from your JavaScript example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1190\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1191\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1193\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1292\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1293\u001b[0m         )\n\u001b[0;32m-> 1294\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit exceeded: free-models-per-day. Add 10 credits to unlock 1000 free model requests per day', 'code': 429, 'metadata': {'headers': {'X-RateLimit-Limit': '50', 'X-RateLimit-Remaining': '0', 'X-RateLimit-Reset': '1770249600000'}, 'provider_name': None}}, 'user_id': 'user_38yXbtpb817LkWFrDul8PhcJH7u'}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ccb27ac0"
      },
      "source": [
        "This Python code now performs the same API call to OpenRouter as your original JavaScript snippet. If you want to integrate this with your existing LangChain summarization agent, you can modify the `create_summarization_agent` function to use `base_url` and `api_key` parameters when initializing `ChatOpenAI`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "OPENROUTER_API_KEY = \"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\"\n",
        "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\""
      ],
      "metadata": {
        "id": "VG03JsGo2P5S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "# LLMChain has been deprecated, using LCEL instead\n",
        "\n",
        "SUMMARY_PROMPT = \"\"\"\n",
        "You are an expert book summarizer.\n",
        "\n",
        "Summarize the following text clearly and concisely.\n",
        "Focus on:\n",
        "- Key ideas\n",
        "- Main arguments\n",
        "- Important outcomes\n",
        "\n",
        "Text:\n",
        "{text}\n",
        "\n",
        "Concise Summary:\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def create_summarization_agent(\n",
        "    model_name=\"tngtech/deepseek-r1t2-chimera:free\",\n",
        "    temperature: float = 0.3,\n",
        "    api_key=OPENROUTER_API_KEY,   # IMPORTANT\n",
        "    api_base=OPENROUTER_BASE_URL  # IMPORTANT\n",
        "):\n",
        "    llm = ChatOpenAI(\n",
        "        model=model_name,\n",
        "        temperature=temperature,\n",
        "        openai_api_base=api_base,\n",
        "        openai_api_key=api_key\n",
        "    )\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"text\"],\n",
        "        template=SUMMARY_PROMPT\n",
        "    )\n",
        "\n",
        "    # LCEL chain\n",
        "    chain = prompt | llm\n",
        "    return chain\n",
        "\n",
        "\n",
        "def summarize_chunks(chunks: list, summarization_chain) -> list:\n",
        "    summaries = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        # Use .invoke() for LCEL chains\n",
        "        response = summarization_chain.invoke({\"text\": chunk})\n",
        "        summary = response.content # Access content from AIMessage or HumanMessage\n",
        "        summaries.append(summary.strip())\n",
        "\n",
        "    return summaries"
      ],
      "metadata": {
        "id": "w3WaYvl3D4u_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "summarization_chain = create_summarization_agent()\n",
        "\n",
        "chapter_summaries = {}\n",
        "\n",
        "for chapter_title, chunks in chunked_chapters.items():\n",
        "    summaries = summarize_chunks(chunks, summarization_chain)\n",
        "    chapter_summaries[chapter_title] = summaries\n",
        "\n",
        "print(chapter_summaries[\"Chapter 1\"][0])\n"
      ],
      "metadata": {
        "id": "3XSmPSelju0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chapter_summaries[\"Chapter 1\"][0])"
      ],
      "metadata": {
        "id": "Bk0-FSEMjyE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHAPTER_SUMMARY_PROMPT = \"\"\"\n",
        "You are an expert book editor.\n",
        "\n",
        "Combine the following partial summaries into a single, coherent chapter summary.\n",
        "- Avoid repetition\n",
        "- Maintain logical flow\n",
        "- Preserve all important ideas\n",
        "\n",
        "Partial Summaries:\n",
        "{summaries}\n",
        "\n",
        "Final Chapter Summary:\n",
        "\"\"\"\n",
        "\n",
        "def create_chapter_summary_agent(\n",
        "    model_name=\"tngtech/deepseek-r1t2-chimera:free\",\n",
        "    temperature=0.3\n",
        "):\n",
        "    llm = ChatOpenAI(\n",
        "        model=model_name,\n",
        "        temperature=temperature,\n",
        "        openai_api_key=OPENROUTER_API_KEY,   # IMPORTANT\n",
        "        openai_api_base=OPENROUTER_BASE_URL  # IMPORTANT\n",
        "    )\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"summaries\"],\n",
        "        template=CHAPTER_SUMMARY_PROMPT\n",
        "    )\n",
        "\n",
        "    return prompt | llm\n",
        "\n",
        "def summarize_chapter(chunk_summaries, chapter_chain):\n",
        "    joined_summaries = \"\\n\".join(\n",
        "        f\"- {summary}\" for summary in chunk_summaries\n",
        "    )\n",
        "\n",
        "    response = chapter_chain.invoke(\n",
        "        {\"summaries\": joined_summaries}\n",
        "    )\n",
        "\n",
        "    return response.content.strip()\n",
        "\n",
        "chapter_summary_chain = create_chapter_summary_agent()\n",
        "\n",
        "final_chapter_summaries = {}\n",
        "\n",
        "for chapter_title, chunk_summaries in chapter_summaries.items():\n",
        "    final_summary = summarize_chapter(\n",
        "        chunk_summaries,\n",
        "        chapter_summary_chain\n",
        "    )\n",
        "    final_chapter_summaries[chapter_title] = final_summary\n",
        "\n",
        "\n",
        "print(final_chapter_summaries[\"Chapter 1\"])\n"
      ],
      "metadata": {
        "id": "8P4dTp_yjx-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BOOK_SUMMARY_PROMPT = \"\"\"\n",
        "You are an expert literary analyst and book summarizer.\n",
        "\n",
        "Using the following chapter summaries, generate:\n",
        "\n",
        "1️⃣ A SHORT SUMMARY (about 1 page)\n",
        "2️⃣ A MEDIUM SUMMARY (5–6 well-structured paragraphs)\n",
        "3️⃣ KEY HIGHLIGHTS (bullet points)\n",
        "\n",
        "Guidelines:\n",
        "- Preserve the core themes and progression of ideas\n",
        "- Avoid repetition\n",
        "- Ensure clarity and coherence\n",
        "- Do NOT invent new content\n",
        "\n",
        "Chapter Summaries:\n",
        "{chapters}\n",
        "\n",
        "Return the output in the following format:\n",
        "\n",
        "SHORT SUMMARY:\n",
        "<text>\n",
        "\n",
        "MEDIUM SUMMARY:\n",
        "<text>\n",
        "\n",
        "KEY HIGHLIGHTS:\n",
        "- bullet 1\n",
        "- bullet 2\n",
        "- bullet 3\n",
        "\"\"\"\n",
        "\n",
        "def create_book_summary_agent(\n",
        "    model_name=\"tngtech/deepseek-r1t2-chimera:free\",\n",
        "    temperature=0.3\n",
        "):\n",
        "    llm = ChatOpenAI(\n",
        "        model=model_name,\n",
        "        temperature=temperature,\n",
        "        openai_api_key=OPENROUTER_API_KEY,\n",
        "        openai_api_base=OPENROUTER_BASE_URL\n",
        "    )\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"chapters\"],\n",
        "        template=BOOK_SUMMARY_PROMPT\n",
        "    )\n",
        "\n",
        "    return prompt | llm\n",
        "def summarize_book(final_chapter_summaries, book_chain):\n",
        "    combined_chapters = \"\\n\\n\".join(\n",
        "        f\"{title}:\\n{summary}\"\n",
        "        for title, summary in final_chapter_summaries.items()\n",
        "    )\n",
        "\n",
        "    response = book_chain.invoke(\n",
        "        {\"chapters\": combined_chapters}\n",
        "    )\n",
        "\n",
        "    return response.content.strip()\n",
        "book_summary_chain = create_book_summary_agent()\n",
        "\n",
        "final_book_summary = summarize_book(\n",
        "    final_chapter_summaries,\n",
        "    book_summary_chain\n",
        ")\n",
        "\n",
        "print(final_book_summary)\n"
      ],
      "metadata": {
        "id": "Zke16gAGjx6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEP 8 :"
      ],
      "metadata": {
        "id": "0hylWj3i9Yd3"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fbd7cde"
      },
      "source": [
        "packages_to_check = ['chromadb', 'langchain', 'langchain_openai', 'tiktoken']\n",
        "not_installed = []\n",
        "\n",
        "for pkg in packages_to_check:\n",
        "    try:\n",
        "        __import__(pkg)\n",
        "        print(f\"{pkg} is already installed.\")\n",
        "    except ImportError:\n",
        "        not_installed.append(pkg)\n",
        "        print(f\"{pkg} is NOT installed.\")\n",
        "\n",
        "if not_installed:\n",
        "    print(f\"\\nTo install the missing packages, run: !pip install {' '.join(not_installed)}\")\n",
        "else:\n",
        "    print(\"\\nAll specified packages are already installed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "embeddings = OpenAIEmbeddings(\n",
        "    model=\"tngtech/deepseek-r1t2-chimera:free\",  # cheap & good\n",
        "    openai_api_key=\"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\",\n",
        ")\n",
        "\n",
        "def chapter_summaries_to_documents(chapter_summaries):\n",
        "    docs = []\n",
        "\n",
        "    for chapter_title, summary in chapter_summaries.items():\n",
        "        doc = Document(\n",
        "            page_content=summary,\n",
        "            metadata={\"chapter\": chapter_title}\n",
        "        )\n",
        "        docs.append(doc)\n",
        "\n",
        "    return docs\n",
        "\n",
        "def store_in_vector_db(documents, persist_dir=\"book_vector_db\"):\n",
        "    vector_db = Chroma.from_documents(\n",
        "        documents=documents,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=persist_dir\n",
        "    )\n",
        "\n",
        "    vector_db.persist()\n",
        "    return vector_db\n",
        "\n",
        "# Convert summaries → documents\n",
        "documents = chapter_summaries_to_documents(final_chapter_summaries)\n",
        "\n",
        "# Store in vector DB\n",
        "vector_db = store_in_vector_db(documents)\n",
        "\n",
        "print(\"✅ Chapter summaries stored in vector database\")\n",
        "\n",
        "query = \"What is the main theme of the book?\"\n",
        "\n",
        "results = vector_db.similarity_search(query, k=3)\n",
        "\n",
        "for res in results:\n",
        "    print(\"Chapter:\", res.metadata[\"chapter\"])\n",
        "    print(res.page_content)\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "id": "YFcAXdbdjx1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m-M91BME7qMZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}